{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PRÁCTICA 2: PROGRAMACIÓN DINÁMICA CON OPENAI GYM Y PYTHON"
      ],
      "metadata": {
        "id": "2H8T1Xar6jIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modificación septiembre 2/2022\n",
        "# Se requiere la versión 0.17.3 de OpenAI Gym para poder\n",
        "# usar el método \"render\". Versiones más recientes no\n",
        "# permiten usar este método desde Google Colab\n",
        "!pip install gym==0.17.3"
      ],
      "metadata": {
        "id": "PfRh_wYH6vdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5d00b7-6216-49ee-b57a-eb3e79648729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.3) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.3) (1.21.6)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654653 sha256=f5b118dd962be5de88d988f5430b46bc641510a8af5f4aa72c7e9d0affc87e51\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.17.3 pyglet-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. El Entorno y la Política\n",
        "\n",
        "Como entorno usaremos [*Frozen Lake*](https://gym.openai.com/envs/FrozenLake-v0/), el mismo  Tablero Bidimensional Estocástico que hemos venido trabajando:"
      ],
      "metadata": {
        "id": "cK8eGXoGWjPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El entorno\n",
        "import gym, numpy as np\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.render()"
      ],
      "metadata": {
        "id": "RTAGROMDWrAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597a46ad-c690-4270-93e9-7b83979459a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y definiremos inicialmente una política totalmente aleatoria: la probabilidad de llegar a la acción \"a\" partiendo del estado \"s\" será la misma para las 4 acciones:"
      ],
      "metadata": {
        "id": "R0fSsWhCW71S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politica_al = np.ones([env.nS, env.nA]) / env.nA\n",
        "print(politica_al.shape)\n",
        "print('Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba')\n",
        "print('-'*60)\n",
        "print('Política aleatoria:')\n",
        "print(politica_al)"
      ],
      "metadata": {
        "id": "2vhzT31rWxXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc7a622-f017-4b35-8451-258ed2a37b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 4)\n",
            "Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba\n",
            "------------------------------------------------------------\n",
            "Política aleatoria:\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Evaluación de la Política\n",
        "\n",
        "Argumentos de entrada:\n",
        "- El entorno\n",
        "- La política: arreglo Numpy de tamaño `nS x nA`. `politica[s][a]` indica la probabilidad de ejecutar la acción `a` partiendo del estado `s`\n",
        "- `gamma`: el factor de descuento\n",
        "- `theta`: criterio de convergencia del algoritmo\n",
        "\n",
        "La función retornará `V`, un vector de `1xnS` con el valor de cada estado.\n",
        "\n",
        "La ecuación de actualización para este algoritmo es:\n",
        "\n",
        "$v_{k+1}(s) = \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$"
      ],
      "metadata": {
        "id": "ly4J0PO_7PbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes\n",
        "GAMMA = 0.99\n",
        "THETA = 1e-10"
      ],
      "metadata": {
        "id": "mtBKOfJ7bTr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluar_politica(env,pol,gamma=GAMMA,theta=THETA):\n",
        "    # Inicializar el vector con los valores de los estados\n",
        "    V = np.zeros(env.nS)\n",
        "\n",
        "    # Iterar y actualizar cada estado. Detener si delta < thetas\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            Vs = 0\n",
        "            for a, pi in enumerate(pol[s]):\n",
        "                for prob, s_, r, done in env.P[s][a]:\n",
        "                    Vs += pi * prob * (r + gamma*V[s_])\n",
        "            delta = max(delta, np.abs(V[s]-Vs))\n",
        "            V[s] = Vs\n",
        "\n",
        "        if delta < theta: # Si converge, detener las iteraciones\n",
        "            break\n",
        "\n",
        "    return V"
      ],
      "metadata": {
        "id": "7_W0oizB7RwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esta función ya podremos evaluar la Política:"
      ],
      "metadata": {
        "id": "nluASA4vC5EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_al = evaluar_politica(env,politica_al)\n",
        "print('Evaluación Política aleatoria:')\n",
        "print(V_al.reshape((4,4)))"
      ],
      "metadata": {
        "id": "jXPVPYWw95hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508e7e4e-0a34-448e-c584-a378b7b347b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluación Política aleatoria:\n",
            "[[0.01235614 0.01042446 0.01933844 0.00947775]\n",
            " [0.01478705 0.         0.03889445 0.        ]\n",
            " [0.03260247 0.08433764 0.13781085 0.        ]\n",
            " [0.         0.17034482 0.43357944 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mejora de la Política\n",
        "\n",
        "Esta requiere el cálculo de la función Acción-Valor ($q_{\\pi}(s,a)$): por cada estado indicará el valor de cada acción que se puede tomar:\n",
        "\n",
        "$q_{\\pi}(s,a)=\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$\n",
        "\n",
        "Por tanto, para un estado en particular, la función Q será un vector de `nA` elementos.\n",
        "\n",
        "Implementaremos inicialmente esta función. Tendrá como argumentos de entrada:\n",
        "- El entorno (`env`)\n",
        "- El factor de descuento (`gamma`)\n",
        "- La función estado-valor (`V`)\n",
        "- El estado sobre el que queremos calcular la función (`s`)\n",
        "\n",
        "A la salida retornará el valor para cada acción:"
      ],
      "metadata": {
        "id": "BymQ7J3oDHoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_funcion_q(env, V, s, gamma=GAMMA):\n",
        "    q = np.zeros(env.nA)\n",
        "    for a in range(env.nA):\n",
        "        for prob, s_, r, done in env.P[s][a]:\n",
        "            q[a] += prob * (r + gamma * V[s_])\n",
        "    return q"
      ],
      "metadata": {
        "id": "UncAl_mg-Szz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y probemos esta función calculándola para cada estado. Por tanto, los valores Q quedarán almacenados en una matriz de `nSxnA`:"
      ],
      "metadata": {
        "id": "zxJag8tYDKou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_al = np.zeros([env.nS, env.nA])\n",
        "\n",
        "for s in range(env.nS):\n",
        "    Q_al[s] = calcular_funcion_q(env, V_al, s)\n",
        "\n",
        "print('Función acción-valor Política Aleatoria')\n",
        "print(Q_al)"
      ],
      "metadata": {
        "id": "1y6VY-we-d1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0219b530-4256-4657-d047-51a8df6beb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función acción-valor Política Aleatoria\n",
            "[[0.01303478 0.01239732 0.01239732 0.01159512]\n",
            " [0.0075176  0.01045921 0.00982176 0.01389928]\n",
            " [0.02265692 0.0194029  0.02234451 0.01294941]\n",
            " [0.00950934 0.00950934 0.00625531 0.012637  ]\n",
            " [0.01971607 0.01563854 0.01483634 0.00895725]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.05185927 0.04547758 0.05185927 0.00638168]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.01563854 0.03859024 0.03271115 0.04346997]\n",
            " [0.06697261 0.11245019 0.10169137 0.0562364 ]\n",
            " [0.18374781 0.17091264 0.15591638 0.04066659]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08404521 0.19929501 0.22712643 0.17091264]\n",
            " [0.24477259 0.53262834 0.52189213 0.43502471]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esta función lista, ya podemos implementar el algoritmo de Mejora de la Política.\n",
        "\n",
        "Esta función tendrá como argumentos de entrada:\n",
        "- El entorno (`env`)\n",
        "- La función estado-valor (`V`) obtenida con la Evaluación de la Política (`evaluar_politica`)\n",
        "- El factor de descuento (`gamma`)\n",
        "\n",
        "A la salida la función retorna la política mejorada, una matriz de `nSxnA`:"
      ],
      "metadata": {
        "id": "bO5OgOkUDSfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mejorar_politica(env, V, gamma=GAMMA):\n",
        "\n",
        "    # Partimos de una Política Arbitraria\n",
        "    politica = np.zeros([env.nS, env.nA])\n",
        "\n",
        "    # Y la mejoramos\n",
        "    for s in range(env.nS):\n",
        "        q = calcular_funcion_q(env, V, s, gamma)\n",
        "        politica[s][np.argmax(q)] = 1\n",
        "\n",
        "    return politica"
      ],
      "metadata": {
        "id": "6lOuMa3NDRTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y probemos la función con el entorno y la función estado-valor calculada anteriormente:"
      ],
      "metadata": {
        "id": "qbNwaijDDbwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politica_mejorada = mejorar_politica(env, V_al)\n",
        "print('Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba')\n",
        "print('-'*60)\n",
        "print('Política aleatoria:')\n",
        "print(politica_al)\n",
        "print('-'*30)\n",
        "\n",
        "print('Evaluación de la Política:')\n",
        "print(V_al)\n",
        "print('-'*30)\n",
        "\n",
        "print('Política mejorada:')\n",
        "print(politica_mejorada)\n",
        "print('-'*30)"
      ],
      "metadata": {
        "id": "GSlHFIumDoX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1f58fe-10d7-4388-cc56-22cb0b248394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba\n",
            "------------------------------------------------------------\n",
            "Política aleatoria:\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "------------------------------\n",
            "Evaluación de la Política:\n",
            "[0.01235614 0.01042446 0.01933844 0.00947775 0.01478705 0.\n",
            " 0.03889445 0.         0.03260247 0.08433764 0.13781085 0.\n",
            " 0.         0.17034482 0.43357944 0.        ]\n",
            "------------------------------\n",
            "Política mejorada:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Iteración de la Política\n",
        "\n",
        "Ahora aplicaremos de forma iterativa la evaluación y la mejora hasta que la Política ya no cambie, es decir hasta tener una Política Óptima.\n",
        "\n",
        "La función aceptará estos argumentos de entrada:\n",
        "- El entorno (`env`)\n",
        "- El factor de descuento (`gamma`)\n",
        "- El parámetro theta (`theta`)\n",
        "\n",
        "Y entregará a la salida una Política Óptima (matriz de `nSxnA`). Además entregará la función valor (`V`) asociada a esa Política (que usaremos como referencia para ver el valor final de cada estado tras la convergencia)"
      ],
      "metadata": {
        "id": "-vOSoOmIgI3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterar_politica(env, gamma=GAMMA, theta=THETA):\n",
        "    politica = np.ones([env.nS, env.nA]) / env.nA # Política arbitraria\n",
        "\n",
        "    # Iterativamente Evaluar->Mejorar hasta que no haya cambios en la Política\n",
        "    while True:\n",
        "        V = evaluar_politica(env, politica)\n",
        "        politica_nueva = mejorar_politica(env, V)\n",
        "\n",
        "        if (politica_nueva == politica).all():\n",
        "            break\n",
        "\n",
        "        politica = np.copy(politica_nueva)\n",
        "\n",
        "    return politica_nueva, V"
      ],
      "metadata": {
        "id": "mMETT-wCdOXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y probemos esta función:"
      ],
      "metadata": {
        "id": "3dYOG62QEuWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politica_it, V_it = iterar_politica(env)"
      ],
      "metadata": {
        "id": "Q0f-yjPchSib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba')\n",
        "print('-'*60)\n",
        "print('Política Óptima Iteración de la Política:')\n",
        "print(politica_it)\n",
        "print('Política original (aleatoria):')\n",
        "print(politica_al)\n",
        "print('Función estado-valor Iteración de la Política:')\n",
        "print(V_it.reshape((4,4)))\n",
        "print('Función estado-valor original (política aleatoria):')\n",
        "print(V_al.reshape((4,4)))"
      ],
      "metadata": {
        "id": "hw-kuKaXDxrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047d4403-45f6-44de-d3e2-14a52ec76955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba\n",
            "------------------------------------------------------------\n",
            "Política Óptima Iteración de la Política:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "Política original (aleatoria):\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "Función estado-valor Iteración de la Política:\n",
            "[[0.54202593 0.49880319 0.47069569 0.4568517 ]\n",
            " [0.55845096 0.         0.35834807 0.        ]\n",
            " [0.59179874 0.64307982 0.61520756 0.        ]\n",
            " [0.         0.74172044 0.86283743 0.        ]]\n",
            "Función estado-valor original (política aleatoria):\n",
            "[[0.01235614 0.01042446 0.01933844 0.00947775]\n",
            " [0.01478705 0.         0.03889445 0.        ]\n",
            " [0.03260247 0.08433764 0.13781085 0.        ]\n",
            " [0.         0.17034482 0.43357944 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Iteración de valores\n",
        "\n",
        "En este caso la evaluación y la mejora se hacen de manera simultánea, en el mismo bloque de iteraciones.\n",
        "\n",
        "Una vez ejecutado este bloque se tendrá la función estado-valor óptima, a partir de la cual se puede obtener una Política Óptima.\n",
        "\n",
        "La función tendrá los mismos argumentos de entrada y de salida que la implementada en la Iteración de la Política:\n",
        "\n",
        "- Entradas:\n",
        "    - El entorno (`env`)\n",
        "    - El factor de descuento (`gamma`)\n",
        "    - El parámetro theta (`theta`)\n",
        "- Salidas:\n",
        "    - Una Política Óptima (matriz de `nSxnA`)\n",
        "    - La función valor (`V`) asociada a esa Política (no es necesaria, pero la retornaremos para tenerla como referencia)\n",
        "\n",
        "La ecuación de actualización es la siguiente:\n",
        "\n",
        "$v_{k+1}(s) \\leftarrow max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$"
      ],
      "metadata": {
        "id": "_d0G1GmQE9Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iteracion_valores(env, gamma=GAMMA, theta=THETA):\n",
        "    V = np.zeros(env.nS)   # Función estado-valor arbitraria\n",
        "\n",
        "    # Obtener función estado-valor óptima\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            v = V[s]   # Almacenar el valor anterior\n",
        "            V[s] = max(calcular_funcion_q(env, V, s))\n",
        "            delta = max(delta,abs(V[s]-v))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Obtener política óptima a partir de la función estado-valor óptima\n",
        "    politica = mejorar_politica(env, V)\n",
        "\n",
        "    return politica, V"
      ],
      "metadata": {
        "id": "UHl4jNlGSJ35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y evaluemos esta función:"
      ],
      "metadata": {
        "id": "K1ZoNA7iTki0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politica_iv, V_iv = iteracion_valores(env)\n",
        "print('Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba')\n",
        "print('-'*60)\n",
        "print('Política óptima (0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba')\n",
        "print(politica_iv)"
      ],
      "metadata": {
        "id": "4R81nTsnTkBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d7b0fc-55f9-40c4-9313-a222a98d9a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acciones: 0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba\n",
            "------------------------------------------------------------\n",
            "Política óptima (0-> Izquierda, 1-> Abajo, 2-> Derecha, 3-> Arriba\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Interactuando con el entorno\n",
        "\n",
        "Para finalizar, haremos que el agente ejecute las acciones definidas por la Política:"
      ],
      "metadata": {
        "id": "NrYg9d7h_1QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = env.reset()\n",
        "\n",
        "for i in range(100):\n",
        "    print(f'Time-step: {i+1}')\n",
        "    a = np.argmax(politica_iv[s])\n",
        "    s_, r, done, info = env.step(a)\n",
        "    s = s_\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    if done:\n",
        "        if s==15:\n",
        "            print('¡EL AGENTE LLEGÓ A LA META!')\n",
        "        else:\n",
        "            print('El agente se encuentra en un estado terminal')\n",
        "        break\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "e3_aSm08-iUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63986927-516e-4975-fbb2-e9fddfef4eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-step: 1\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 2\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 3\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 4\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 5\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 6\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 7\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 8\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 9\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 10\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 11\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 12\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 13\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 14\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Time-step: 15\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Time-step: 16\n",
            "  (Left)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 17\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 18\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 19\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 20\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 21\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 22\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 23\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 24\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 25\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 26\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 27\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 28\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 29\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 30\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 31\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 32\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 33\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Time-step: 34\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 35\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 36\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 37\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 38\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 39\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 40\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 41\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 42\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 43\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 44\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 45\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 46\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 47\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 48\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 49\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Time-step: 50\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Time-step: 51\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Time-step: 52\n",
            "  (Left)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "Time-step: 53\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "El agente se encuentra en un estado terminal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g750WVvtcw-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}