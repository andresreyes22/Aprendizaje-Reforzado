{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjkYGFuaEBrn",
        "outputId": "a37f2fa4-8aee-476f-e7be-0eaca37d82fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Política aprendida:\n",
            "[['r' 'r' 'd' 'r']\n",
            " ['u' 'H' 'd' 'H']\n",
            " ['r' 'r' 'd' 'H']\n",
            " ['H' 'r' 'r' 'G']]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Configuración inicial\n",
        "gridworld = np.array([\n",
        "    ['S', 'F', 'F', 'F'],\n",
        "    ['F', 'H', 'F', 'H'],\n",
        "    ['F', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'G']\n",
        "])\n",
        "\n",
        "# Parámetros del entorno y del aprendizaje\n",
        "gamma = 0.9  # Factor de descuento\n",
        "epsilon = 0.1  # Parámetro de exploración\n",
        "episodes = 100  # Número de episodios\n",
        "alpha = 0.1  # Tasa de aprendizaje\n",
        "\n",
        "# Mapear celdas a recompensas\n",
        "rewards = {\n",
        "    'S': 0,  # Inicio\n",
        "    'F': 0,  # Estado regular\n",
        "    'H': -1,  # Hoyo\n",
        "    'G': 1   # Meta\n",
        "}\n",
        "\n",
        "# Dimensiones del mundo\n",
        "n_rows, n_cols = gridworld.shape\n",
        "\n",
        "# Acciones posibles: arriba, abajo, izquierda, derecha\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_effects = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "\n",
        "# Función para validar movimientos\n",
        "def is_valid_move(row, col):\n",
        "    return 0 <= row < n_rows and 0 <= col < n_cols\n",
        "\n",
        "# Inicializar valores Q(s, a)\n",
        "Q = {}\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        Q[(row, col)] = {action: 0 for action in actions}\n",
        "\n",
        "# Epsilon-greedy policy\n",
        "def epsilon_greedy(state):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)  # Explorar\n",
        "    else:\n",
        "        # Elegir acción con mayor valor Q\n",
        "        state_actions = Q[state]\n",
        "        return max(state_actions, key=state_actions.get)\n",
        "\n",
        "# Simular un episodio\n",
        "def generate_episode():\n",
        "    # Estado inicial\n",
        "    row, col = np.where(gridworld == 'S')\n",
        "    state = (row[0], col[0])\n",
        "    episode = []\n",
        "\n",
        "    while True:\n",
        "        action = epsilon_greedy(state)\n",
        "        # Aplicar acción\n",
        "        new_row = state[0] + action_effects[action][0]\n",
        "        new_col = state[1] + action_effects[action][1]\n",
        "        if is_valid_move(new_row, new_col):\n",
        "            next_state = (new_row, new_col)\n",
        "        else:\n",
        "            next_state = state  # Movimiento inválido, quedarse en el mismo lugar\n",
        "\n",
        "        # Registrar la transición\n",
        "        reward = rewards[gridworld[next_state]]\n",
        "        episode.append((state, action, reward))\n",
        "\n",
        "        # Verificar si el episodio termina\n",
        "        if gridworld[next_state] in ['G', 'H']:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    return episode\n",
        "\n",
        "# Algoritmo Monte Carlo\n",
        "for episode in range(episodes):\n",
        "    # Generar un episodio\n",
        "    episode_data = generate_episode()\n",
        "\n",
        "    # Calcular el retorno acumulado (G_t) y actualizar Q\n",
        "    G = 0\n",
        "    visited = set()\n",
        "    for state, action, reward in reversed(episode_data):\n",
        "        G = reward + gamma * G\n",
        "        if (state, action) not in visited:\n",
        "            visited.add((state, action))\n",
        "            # Actualizar Q(s, a)\n",
        "            Q[state][action] += alpha * (G - Q[state][action])\n",
        "\n",
        "# Visualizar la política aprendida\n",
        "policy = np.empty_like(gridworld, dtype=str)\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        state = (row, col)\n",
        "        if gridworld[state] in ['H', 'G']:\n",
        "            policy[state] = gridworld[state]\n",
        "        else:\n",
        "            policy[state] = max(Q[state], key=Q[state].get)\n",
        "\n",
        "print(\"Política aprendida:\")\n",
        "print(policy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisión en 1 dimensión"
      ],
      "metadata": {
        "id": "F8uc1lB8JU5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Configuración inicial\n",
        "gridworld = np.array([\n",
        "    ['H','F','S', 'F',  'G'],\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "2tsTaYvAJUVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros del entorno y del aprendizaje\n",
        "gamma = 0.9  # Factor de descuento\n",
        "epsilon = 0.1  # Parámetro de exploración\n",
        "episodes = 2  # Número de episodios\n",
        "alpha = 0.1  # Tasa de aprendizaje\n",
        "\n"
      ],
      "metadata": {
        "id": "rk38OGaWKgpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapear celdas a recompensas\n",
        "rewards = {\n",
        "    'S': 0,  # Inicio\n",
        "    'F': 0,  # Estado regular\n",
        "    'H': -1,  # Hoyo\n",
        "    'G': 1   # Meta\n",
        "}"
      ],
      "metadata": {
        "id": "3ILKcYqPKnAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensiones del mundo\n",
        "n_rows, n_cols = gridworld.shape\n",
        "\n",
        "# Acciones posibles: arriba, abajo, izquierda, derecha\n",
        "actions = ['left', 'right']\n",
        "action_effects = {\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}"
      ],
      "metadata": {
        "id": "fu_bk4RDKqfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para validar movimientos\n",
        "def is_valid_move(row, col):\n",
        "    return 0 <= row < n_rows and 0 <= col < n_cols"
      ],
      "metadata": {
        "id": "15Owq4UqKuUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar valores Q(s, a)\n",
        "Q = {}\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        Q[(row, col)] = {action: 0 for action in actions}\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD2KAm-fKxYV",
        "outputId": "eea2aeb9-ee57-43d0-8e58-46f52decc442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(0, 0): {'left': 0, 'right': 0}, (0, 1): {'left': 0, 'right': 0}, (0, 2): {'left': 0, 'right': 0}, (0, 3): {'left': 0, 'right': 0}, (0, 4): {'left': 0, 'right': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-greedy policy\n",
        "def epsilon_greedy(state):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)  # Explorar\n",
        "    else:\n",
        "        # Elegir acción con mayor valor Q\n",
        "        state_actions = Q[state]\n",
        "        return max(state_actions, key=state_actions.get)\n"
      ],
      "metadata": {
        "id": "znIqYZUoKz18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simular un episodio\n",
        "def generate_episode():\n",
        "    # Estado inicial\n",
        "    row, col = np.where(gridworld == 'S')\n",
        "    print(\"np.where(gridworld == 'S') , row=\",str(row),\" col= \",str(col))\n",
        "    state = (row[0], col[0])\n",
        "    episode = []\n",
        "\n",
        "    while True:\n",
        "        action = epsilon_greedy(state)\n",
        "        # Aplicar acción\n",
        "        new_row = 0\n",
        "        new_col = state[1] + action_effects[action][1]\n",
        "        if is_valid_move(new_row, new_col):\n",
        "            next_state = (new_row, new_col)\n",
        "        else:\n",
        "            next_state = state  # Movimiento inválido, quedarse en el mismo lugar\n",
        "\n",
        "        # Registrar la transición\n",
        "        reward = rewards[gridworld[next_state]]\n",
        "        episode.append((state, action, reward))\n",
        "\n",
        "        # Verificar si el episodio termina\n",
        "        if gridworld[next_state] in ['G', 'H']:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    return episode\n",
        "\n"
      ],
      "metadata": {
        "id": "jU6ce7GrKkdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algoritmo Monte Carlo\n",
        "for episode in range(episodes):\n",
        "    # Generar un episodio\n",
        "    episode_data = generate_episode()\n",
        "\n",
        "    # Calcular el retorno acumulado (G_t) y actualizar Q\n",
        "    G = 0\n",
        "    visited = set()\n",
        "    for state, action, reward in reversed(episode_data):\n",
        "        G = reward + gamma * G\n",
        "        if (state, action) not in visited:\n",
        "            visited.add((state, action))\n",
        "            # Actualizar Q(s, a)\n",
        "            Q[state][action] += alpha * (G - Q[state][action])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBS-cSZyK5c6",
        "outputId": "fc3c1ea4-f291-4323-ec4a-eb85e00817bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np.where(gridworld == 'S') , row= [0]  col=  [2]\n",
            "np.where(gridworld == 'S') , row= [0]  col=  [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Visualizar la política aprendida\n",
        "policy = np.empty_like(gridworld, dtype=str)\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        state = (row, col)\n",
        "        if gridworld[state] in ['H', 'G']:\n",
        "            policy[state] = gridworld[state]\n",
        "        else:\n",
        "            policy[state] = max(Q[state], key=Q[state].get)\n",
        "\n",
        "print(\"Política aprendida:\")\n",
        "print(policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmF576ybK2wU",
        "outputId": "95082d73-7a9e-4eac-eb09-ed6cf97387ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Política aprendida:\n",
            "[['H' 'r' 'r' 'r' 'G']]\n"
          ]
        }
      ]
    }
  ]
}